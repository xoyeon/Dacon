{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lv4 íŠœë‹ 5/7 ëª¨ë¸ íŠœë‹ ë° Voting Classifier",
      "provenance": [],
      "collapsed_sections": [
        "gU02qWuqR24f",
        "KJVABGt5SfYg",
        "_05EetCjSvoe",
        "_h_W13Lc-p82"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xoyeon/Dacon-Daily-Python-Camp/blob/main/Lv4_%ED%8A%9C%EB%8B%9D_5_7_%EB%AA%A8%EB%8D%B8_%ED%8A%9C%EB%8B%9D_%EB%B0%8F_Voting_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIpjpOEXsoW0"
      },
      "source": [
        "# [â†©ï¸ ë¦¬ìŠ¤íŠ¸ë¡œ ëŒì•„ê°€ê¸°](https://dacon.io/competitions/open/235698/overview/description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdkhATahdXka"
      },
      "source": [
        "# ì…€ì„ ì‹¤í–‰í•  ë•Œ íŒì—…ì´ ëœ¨ë©´ `ë¬´ì‹œí•˜ê³  ê³„ì†í•˜ê¸°` ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91j6o0CC8y6w"
      },
      "source": [
        "## ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
        "---\n",
        "ì•„ë˜ ì…€ì„ ì‹¤í–‰ì‹œì¼œ ë°ì´í„°ë¥¼ colab ì— ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
        "ì…€ ì‹¤í–‰ì€ Ctrl + Enter ë¥¼ ì´ìš©í•´ ì‹¤í–‰ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVPJmIjs82WW",
        "outputId": "71bff78a-0375-41ca-f396-c57a1e7732ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# ë°ì´í„° ë‹¤ìš´ë¡œë“œ ë§í¬ë¡œ ë°ì´í„°ë¥¼ ì½”ë©ì— ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
        "\n",
        "!wget 'https://bit.ly/3i4n1QB'\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile('3i4n1QB', 'r') as existing_zip:\n",
        "    existing_zip.extractall('data')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-06 10:52:22--  https://bit.ly/3i4n1QB\n",
            "Resolving bit.ly (bit.ly)... 67.199.248.11, 67.199.248.10\n",
            "Connecting to bit.ly (bit.ly)|67.199.248.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://drive.google.com/uc?export=download&id=1emLrrpFWT8dCoj5BJb12-5QMG2-nruUw [following]\n",
            "--2021-09-06 10:52:22--  https://drive.google.com/uc?export=download&id=1emLrrpFWT8dCoj5BJb12-5QMG2-nruUw\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.13.238, 2607:f8b0:4004:80a::200e\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.13.238|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-10-10-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/702b8n79tqenn4fftk01n1neems64o51/1630925475000/17946651057176172524/*/1emLrrpFWT8dCoj5BJb12-5QMG2-nruUw?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-09-06 10:52:23--  https://doc-10-10-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/702b8n79tqenn4fftk01n1neems64o51/1630925475000/17946651057176172524/*/1emLrrpFWT8dCoj5BJb12-5QMG2-nruUw?e=download\n",
            "Resolving doc-10-10-docs.googleusercontent.com (doc-10-10-docs.googleusercontent.com)... 142.250.65.65, 2607:f8b0:4004:832::2001\n",
            "Connecting to doc-10-10-docs.googleusercontent.com (doc-10-10-docs.googleusercontent.com)|142.250.65.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 137694 (134K) [application/zip]\n",
            "Saving to: â€˜3i4n1QBâ€™\n",
            "\n",
            "3i4n1QB             100%[===================>] 134.47K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-09-06 10:52:23 (5.65 MB/s) - â€˜3i4n1QBâ€™ saved [137694/137694]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqVbFVMCGJzT",
        "outputId": "382c9133-f3d6-4184-bf1c-b5d8a85fdb49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ í•™ìŠµì‹œí‚¬ ì¤€ë¹„í•˜ê¸°\n",
        "\n",
        "train = pd.read_csv('data/train.csv')\n",
        "test = pd.read_csv('data/test.csv')\n",
        "\n",
        "# Scailing\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train[['fixed acidity']])\n",
        "train['Scaled fixed acidity'] = scaler.transform(train[['fixed acidity']])\n",
        "test['Scaled fixed acidity'] = scaler.transform(test[['fixed acidity']])\n",
        "\n",
        "# Encoding\n",
        "encoder = OneHotEncoder()\n",
        "encoder.fit(train[['type']])\n",
        "onehot = encoder.transform(train[['type']])\n",
        "onehot = onehot.toarray()\n",
        "onehot = pd.DataFrame(onehot)\n",
        "onehot.columns = encoder.get_feature_names()\n",
        "train = pd.concat([train, onehot], axis = 1)\n",
        "train = train.drop(columns = ['type'])\n",
        "\n",
        "onehot = encoder.transform(test[['type']])\n",
        "onehot = onehot.toarray()\n",
        "onehot = pd.DataFrame(onehot)\n",
        "onehot.columns = encoder.get_feature_names()\n",
        "test = pd.concat([test, onehot], axis = 1)\n",
        "test = test.drop(columns = ['type'])\n",
        "\n",
        "test.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>Scaled fixed acidity</th>\n",
              "      <th>x0_red</th>\n",
              "      <th>x0_white</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.48</td>\n",
              "      <td>6.6</td>\n",
              "      <td>0.043</td>\n",
              "      <td>11.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>0.99380</td>\n",
              "      <td>2.90</td>\n",
              "      <td>0.38</td>\n",
              "      <td>11.6</td>\n",
              "      <td>0.429752</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>13.3</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.58</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.070</td>\n",
              "      <td>15.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.00040</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.49</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.785124</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>6.5</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.27</td>\n",
              "      <td>5.2</td>\n",
              "      <td>0.040</td>\n",
              "      <td>44.0</td>\n",
              "      <td>179.0</td>\n",
              "      <td>0.99480</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.69</td>\n",
              "      <td>9.4</td>\n",
              "      <td>0.223140</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>7.2</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.043</td>\n",
              "      <td>21.0</td>\n",
              "      <td>159.0</td>\n",
              "      <td>0.99480</td>\n",
              "      <td>3.52</td>\n",
              "      <td>0.47</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.280992</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>6.8</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.26</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.019</td>\n",
              "      <td>23.5</td>\n",
              "      <td>72.0</td>\n",
              "      <td>0.99041</td>\n",
              "      <td>3.16</td>\n",
              "      <td>0.47</td>\n",
              "      <td>11.8</td>\n",
              "      <td>0.247934</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  fixed acidity  ...  x0_red  x0_white\n",
              "0      0            9.0  ...     0.0       1.0\n",
              "1      1           13.3  ...     1.0       0.0\n",
              "2      2            6.5  ...     0.0       1.0\n",
              "3      3            7.2  ...     0.0       1.0\n",
              "4      4            6.8  ...     0.0       1.0\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWUuPtvUZ4zO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f46679dc-9a75-478a-f8d9-8b9d887c372e"
      },
      "source": [
        "pip install bayesian-optimization"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian-optimization-1.2.0.tar.gz (14 kB)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.0.1)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-py3-none-any.whl size=11685 sha256=855605f311a3409e5627c214bb541642ab42f6f9bc35599411ff1ab42f928a15\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/9b/71/f127d694e02eb40bcf18c7ae9613b88a6be4470f57a8528c5b\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5MhylK9Z7Ov"
      },
      "source": [
        "# Bayesian Optimization ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "from bayes_opt import BayesianOptimization"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZksEbPou52yW"
      },
      "source": [
        "## ğŸ‘‹ ì‹¤ìŠµ\n",
        "---\n",
        "ì‹¤ìŠµ ìˆœì„œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "1. Random forest íŠœë‹\n",
        "2. XGBoost íŠœë‹\n",
        "3. Light GBM íŠœë‹\n",
        "4. Voting Claasifier ìƒì„±\n",
        "5. Voting Claasifier í•™ìŠµ ë° ì˜ˆì¸¡\n",
        "---\n",
        "ì´ë²ˆì‹œê°„ì—ëŠ” ì‹¤ìŠµ ëŸ‰ì´ ë§ì•„ Voting Classifier ìƒì„±ê¹Œì§€ë§Œ ì§„í–‰í•˜ê³  ë‹¤ìŒ ì‹œê°„ì— ì´ì–´ì„œ ì§„í–‰ í•  ì˜ˆì •ì…ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU02qWuqR24f"
      },
      "source": [
        "### Random forest íŠœë‹\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41v3TTXtR2uh",
        "outputId": "77b6c1a0-7329-4353-ba4e-f84a566e5b2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Xì— í•™ìŠµí•  ë°ì´í„°ë¥¼, yì— ëª©í‘œ ë³€ìˆ˜ë¥¼ ì €ì¥í•´ì£¼ì„¸ìš”\n",
        "\n",
        "X = train.drop(columns = ['index', 'quality'])\n",
        "\n",
        "y = train['quality']\n",
        "\n",
        "\n",
        "\n",
        "# ëœë¤í¬ë ˆìŠ¤íŠ¸ì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì˜ ë²”ìœ„ë¥¼ dictionary í˜•íƒœë¡œ ì§€ì •í•´ì£¼ì„¸ìš”\n",
        "\n",
        "## KeyëŠ” ëœë¤í¬ë ˆìŠ¤íŠ¸ì˜ hyperparameterì´ë¦„ì´ê³ , valueëŠ” íƒìƒ‰í•  ë²”ìœ„ ì…ë‹ˆë‹¤.\n",
        "\n",
        "rf_parameter_bounds = {\n",
        "\n",
        "                      'max_depth' : (1,3), # ë‚˜ë¬´ì˜ ê¹Šì´\n",
        "\n",
        "                      'n_estimators' : (30,100),\n",
        "\n",
        "                      }\n",
        "\n",
        "\n",
        "\n",
        "# í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì£¼ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "# í•¨ìˆ˜ì˜ êµ¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "\n",
        "# 1. í•¨ìˆ˜ì— ë“¤ì–´ê°€ëŠ” ì¸ì = ìœ„ì—ì„œ ë§Œë“  í•¨ìˆ˜ì˜ keyê°’ë“¤\n",
        "\n",
        "# 2. í•¨ìˆ˜ ì† ì¸ìë¥¼ í†µí•´ ë°›ì•„ì™€ ìƒˆë¡­ê²Œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
        "\n",
        "# 3. ê·¸ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ ìƒì„±\n",
        "\n",
        "# 4. train_test_splitì„ í†µí•´ ë°ì´í„° train-valid ë‚˜ëˆ„ê¸°\n",
        "\n",
        "# 5 .ëª¨ë¸ í•™ìŠµ\n",
        "\n",
        "# 6. ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •\n",
        "\n",
        "# 7. ëª¨ë¸ì˜ ì ìˆ˜ ë°˜í™˜\n",
        "\n",
        "\n",
        "\n",
        "def rf_bo(max_depth, n_estimators):\n",
        "  #####################################\n",
        " \n",
        "  rf_params = {\n",
        "\n",
        "              'max_depth' : int(round(max_depth)),\n",
        "\n",
        "               'n_estimators' : int(round(n_estimators)),      \n",
        "\n",
        "              }\n",
        "\n",
        "  rf = RandomForestClassifier(**rf_params)\n",
        "\n",
        "\n",
        "\n",
        "  X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size = 0.2, )\n",
        "\n",
        "\n",
        "\n",
        "  rf.fit(X_train,y_train)\n",
        "\n",
        "  score = accuracy_score(y_valid, rf.predict(X_valid))\n",
        "\n",
        "  return score\n",
        "\n",
        "  #####################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ì´ì œ Bayesian Optimizationì„ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ëë‚¬ìŠµë‹ˆë‹¤.\n",
        "\n",
        "# \"BO_rf\"ë¼ëŠ” ë³€ìˆ˜ì— Bayesian Optmizationì„ ì €ì¥í•´ë³´ì„¸ìš”\n",
        "\n",
        "BO_rf = BayesianOptimization(f = rf_bo, pbounds = rf_parameter_bounds,random_state = 0)\n",
        "\n",
        "\n",
        "\n",
        "# Bayesian Optimizationì„ ì‹¤í–‰í•´ë³´ì„¸ìš”\n",
        "\n",
        "BO_rf.maximize(init_points = 5, n_iter = 5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | max_depth | n_esti... |\n",
            "-------------------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5073  \u001b[0m | \u001b[0m 2.098   \u001b[0m | \u001b[0m 80.06   \u001b[0m |\n",
            "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.5273  \u001b[0m | \u001b[95m 2.206   \u001b[0m | \u001b[95m 68.14   \u001b[0m |\n",
            "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.5155  \u001b[0m | \u001b[0m 1.847   \u001b[0m | \u001b[0m 75.21   \u001b[0m |\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5145  \u001b[0m | \u001b[0m 1.875   \u001b[0m | \u001b[0m 92.42   \u001b[0m |\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.5273  \u001b[0m | \u001b[0m 2.927   \u001b[0m | \u001b[0m 56.84   \u001b[0m |\n",
            "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.4291  \u001b[0m | \u001b[0m 1.106   \u001b[0m | \u001b[0m 30.01   \u001b[0m |\n",
            "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.4355  \u001b[0m | \u001b[0m 1.024   \u001b[0m | \u001b[0m 71.5    \u001b[0m |\n",
            "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.5355  \u001b[0m | \u001b[95m 3.0     \u001b[0m | \u001b[95m 100.0   \u001b[0m |\n",
            "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.5418  \u001b[0m | \u001b[95m 3.0     \u001b[0m | \u001b[95m 41.58   \u001b[0m |\n",
            "| \u001b[95m 10      \u001b[0m | \u001b[95m 0.5509  \u001b[0m | \u001b[95m 3.0     \u001b[0m | \u001b[95m 72.73   \u001b[0m |\n",
            "=================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJVABGt5SfYg"
      },
      "source": [
        "### XGBoost íŠœë‹"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pWpHl1LdeEp",
        "outputId": "84952271-bb1d-4f79-988f-dbe15523e14e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Xì— í•™ìŠµí•  ë°ì´í„°ë¥¼, yì— ëª©í‘œ ë³€ìˆ˜ë¥¼ ì €ì¥í•´ì£¼ì„¸ìš”\n",
        "\n",
        "X = train.drop(columns = ['index', 'quality'])\n",
        "\n",
        "y = train['quality']\n",
        "\n",
        "\n",
        "\n",
        "# XGBoostì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì˜ ë²”ìœ„ë¥¼ dictionary í˜•íƒœë¡œ ì§€ì •í•´ì£¼ì„¸ìš”\n",
        "\n",
        "## KeyëŠ” XGBoost hyperparameterì´ë¦„ì´ê³ , valueëŠ” íƒìƒ‰í•  ë²”ìœ„ ì…ë‹ˆë‹¤.\n",
        "\n",
        "xgb_parameter_bounds = {\n",
        "\n",
        "                      'gamma' : (0,10),\n",
        "\n",
        "                      'max_depth' : (1,3), # ë‚˜ë¬´ì˜ ê¹Šì´\n",
        "\n",
        "                      'subsample' : (0.5,1)\n",
        "\n",
        "                      }\n",
        "\n",
        "\n",
        "\n",
        "# í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì£¼ê² ìŠµë‹ˆë‹¤.\n",
        "# í•¨ìˆ˜ì˜ êµ¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "# 1. í•¨ìˆ˜ì— ë“¤ì–´ê°€ëŠ” ì¸ì = ìœ„ì—ì„œ ë§Œë“  í•¨ìˆ˜ì˜ keyê°’ë“¤\n",
        "# 2. í•¨ìˆ˜ ì† ì¸ìë¥¼ í†µí•´ ë°›ì•„ì™€ ìƒˆë¡­ê²Œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
        "# 3. ê·¸ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ ìƒì„±\n",
        "# 4. train_test_splitì„ í†µí•´ ë°ì´í„° train-valid ë‚˜ëˆ„ê¸°\n",
        "# 5 .ëª¨ë¸ í•™ìŠµ\n",
        "# 6. ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •\n",
        "# 7. ëª¨ë¸ì˜ ì ìˆ˜ ë°˜í™˜\n",
        "\n",
        "\n",
        "\n",
        "def xgb_bo(gamma,max_depth, subsample):\n",
        "\n",
        "  #####################################\n",
        "  xgb_params = {\n",
        "\n",
        "              'gamma' : int(round(gamma)),\n",
        "\n",
        "              'max_depth' : int(round(max_depth)),\n",
        "\n",
        "               'subsample' : int(round(subsample)),      \n",
        "\n",
        "              }\n",
        "  xgb = XGBClassifier(**xgb_params)\n",
        "\n",
        "\n",
        "\n",
        "  X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size = 0.2, )\n",
        "\n",
        "\n",
        "\n",
        "  xgb.fit(X_train,y_train)\n",
        "\n",
        "  score = accuracy_score(y_valid, xgb.predict(X_valid))\n",
        "\n",
        "  return score\n",
        "  #####################################\n",
        "\n",
        "\n",
        "\n",
        "# ì´ì œ Bayesian Optimizationì„ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ëë‚¬ìŠµë‹ˆë‹¤.\n",
        "\n",
        "# \"BO_xgb\"ë¼ëŠ” ë³€ìˆ˜ì— Bayesian Optmizationì„ ì €ì¥í•´ë³´ì„¸ìš”\n",
        "\n",
        "BO_xgb = BayesianOptimization(f = xgb_bo, pbounds = xgb_parameter_bounds,random_state = 0)\n",
        "\n",
        "\n",
        "\n",
        "# Bayesian Optimizationì„ ì‹¤í–‰í•´ë³´ì„¸ìš”\n",
        "\n",
        "BO_xgb.maximize(init_points = 5, n_iter = 5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   |   gamma   | max_depth | subsample |\n",
            "-------------------------------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5309  \u001b[0m | \u001b[0m 5.488   \u001b[0m | \u001b[0m 2.43    \u001b[0m | \u001b[0m 0.8014  \u001b[0m |\n",
            "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.5255  \u001b[0m | \u001b[0m 5.449   \u001b[0m | \u001b[0m 1.847   \u001b[0m | \u001b[0m 0.8229  \u001b[0m |\n",
            "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.5691  \u001b[0m | \u001b[95m 4.376   \u001b[0m | \u001b[95m 2.784   \u001b[0m | \u001b[95m 0.9818  \u001b[0m |\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5445  \u001b[0m | \u001b[0m 3.834   \u001b[0m | \u001b[0m 2.583   \u001b[0m | \u001b[0m 0.7644  \u001b[0m |\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.5573  \u001b[0m | \u001b[0m 5.68    \u001b[0m | \u001b[0m 2.851   \u001b[0m | \u001b[0m 0.5355  \u001b[0m |\n",
            "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.000909\u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m |\n",
            "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.5445  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
            "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.5891  \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 3.0     \u001b[0m | \u001b[95m 1.0     \u001b[0m |\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.5382  \u001b[0m | \u001b[0m 2.343   \u001b[0m | \u001b[0m 1.026   \u001b[0m | \u001b[0m 0.9694  \u001b[0m |\n",
            "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.004545\u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 3.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m |\n",
            "=============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_05EetCjSvoe"
      },
      "source": [
        "### LGBM íŠœë‹\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qH2nrvlSx6I",
        "outputId": "c813afd2-c6a5-4dbb-e406-a35e52be8bca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Xì— í•™ìŠµí•  ë°ì´í„°ë¥¼, yì— ëª©í‘œ ë³€ìˆ˜ë¥¼ ì €ì¥í•´ì£¼ì„¸ìš”\n",
        "\n",
        "X = train.drop(columns = ['index', 'quality'])\n",
        "\n",
        "y = train['quality']\n",
        "\n",
        "\n",
        "\n",
        "# LGBMì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì˜ ë²”ìœ„ë¥¼ dictionary í˜•íƒœë¡œ ì§€ì •í•´ì£¼ì„¸ìš”\n",
        "\n",
        "## KeyëŠ” LGBM hyperparameterì´ë¦„ì´ê³ , valueëŠ” íƒìƒ‰í•  ë²”ìœ„ ì…ë‹ˆë‹¤.\n",
        "\n",
        "lgbm_parameter_bounds = {\n",
        "\n",
        "                      'n_estimators' : (30,100),\n",
        "\n",
        "                      'max_depth' : (1,3), # ë‚˜ë¬´ì˜ ê¹Šì´\n",
        "\n",
        "                      'subsample' : (0.5,1)\n",
        "\n",
        "                      }\n",
        "\n",
        "\n",
        "\n",
        "# í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì£¼ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "# í•¨ìˆ˜ì˜ êµ¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "\n",
        "# 1. í•¨ìˆ˜ì— ë“¤ì–´ê°€ëŠ” ì¸ì = ìœ„ì—ì„œ ë§Œë“  í•¨ìˆ˜ì˜ keyê°’ë“¤\n",
        "\n",
        "# 2. í•¨ìˆ˜ ì† ì¸ìë¥¼ í†µí•´ ë°›ì•„ì™€ ìƒˆë¡­ê²Œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
        "\n",
        "# 3. ê·¸ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ ìƒì„±\n",
        "\n",
        "# 4. train_test_splitì„ í†µí•´ ë°ì´í„° train-valid ë‚˜ëˆ„ê¸°\n",
        "\n",
        "# 5. ëª¨ë¸ í•™ìŠµ\n",
        "\n",
        "# 6. ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •\n",
        "\n",
        "# 7. ëª¨ë¸ì˜ ì ìˆ˜ ë°˜í™˜\n",
        "\n",
        "\n",
        "\n",
        "def lgbm_bo(n_estimators,max_depth, subsample):\n",
        "\n",
        "  #####################################\n",
        "  lgbm_params = {\n",
        "\n",
        "              'n_estimators' : int(round(n_estimators)),\n",
        "\n",
        "              'max_depth' : int(round(max_depth)),\n",
        "\n",
        "               'subsample' : int(round(subsample)),      \n",
        "\n",
        "              }\n",
        "  lgbm = LGBMClassifier(**lgbm_params)\n",
        "\n",
        "\n",
        "\n",
        "  X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size = 0.2, )\n",
        "\n",
        "\n",
        "\n",
        "  lgbm.fit(X_train,y_train)\n",
        "\n",
        "  score = accuracy_score(y_valid, lgbm.predict(X_valid))\n",
        "\n",
        "  return score\n",
        "\n",
        "\n",
        "  #####################################\n",
        "\n",
        "\n",
        "\n",
        "# ì´ì œ Bayesian Optimizationì„ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ëë‚¬ìŠµë‹ˆë‹¤.\n",
        "\n",
        "# \"BO_lgbm\"ë¼ëŠ” ë³€ìˆ˜ì— Bayesian Optmizationì„ ì €ì¥í•´ë³´ì„¸ìš”\n",
        "\n",
        "BO_lgbm = BayesianOptimization(f = lgbm_bo, pbounds = lgbm_parameter_bounds,random_state = 0)\n",
        "\n",
        "\n",
        "\n",
        "# Bayesian Optimizationì„ ì‹¤í–‰í•´ë³´ì„¸ìš”\n",
        "BO_lgbm.maximize(init_points = 5, n_iter = 5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | max_depth | n_esti... | subsample |\n",
            "-------------------------------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.56    \u001b[0m | \u001b[0m 2.098   \u001b[0m | \u001b[0m 80.06   \u001b[0m | \u001b[0m 0.8014  \u001b[0m |\n",
            "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.5591  \u001b[0m | \u001b[0m 2.09    \u001b[0m | \u001b[0m 59.66   \u001b[0m | \u001b[0m 0.8229  \u001b[0m |\n",
            "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.5409  \u001b[0m | \u001b[0m 1.875   \u001b[0m | \u001b[0m 92.42   \u001b[0m | \u001b[0m 0.9818  \u001b[0m |\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5464  \u001b[0m | \u001b[0m 1.767   \u001b[0m | \u001b[0m 85.42   \u001b[0m | \u001b[0m 0.7644  \u001b[0m |\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.5309  \u001b[0m | \u001b[0m 2.136   \u001b[0m | \u001b[0m 94.79   \u001b[0m | \u001b[0m 0.5355  \u001b[0m |\n",
            "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.5445  \u001b[0m | \u001b[0m 2.951   \u001b[0m | \u001b[0m 30.14   \u001b[0m | \u001b[0m 0.9851  \u001b[0m |\n",
            "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.5636  \u001b[0m | \u001b[95m 2.975   \u001b[0m | \u001b[95m 70.17   \u001b[0m | \u001b[95m 0.9819  \u001b[0m |\n",
            "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.5345  \u001b[0m | \u001b[0m 1.092   \u001b[0m | \u001b[0m 42.31   \u001b[0m | \u001b[0m 0.9997  \u001b[0m |\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.5545  \u001b[0m | \u001b[0m 1.004   \u001b[0m | \u001b[0m 70.16   \u001b[0m | \u001b[0m 0.9359  \u001b[0m |\n",
            "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.5609  \u001b[0m | \u001b[0m 2.995   \u001b[0m | \u001b[0m 51.18   \u001b[0m | \u001b[0m 0.883   \u001b[0m |\n",
            "=============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF3qStQlS-G3"
      },
      "source": [
        "### Voting Classifier ìƒì„±\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMd_ySPETMcr"
      },
      "source": [
        "# ëª¨ë¸ ì •ì˜ (íŠœë‹ëœ íŒŒë¼ë¯¸í„°ë¡œ)\n",
        "LGBM = LGBMClassifier(max_depth = 2.09,n_estimators=60, subsample = 0.8229)\n",
        "XGB = XGBClassifier(gamma =  4.376, max_depth = 2.784, subsample = 0.9818)\n",
        "RF = RandomForestClassifier(max_depth = 3.0, n_estimators = 35.31)\n",
        "\n",
        "# VotingClassifier ì •ì˜\n",
        "VC = VotingClassifier(estimators=[('rf',RF),('xgb',XGB),('lgbm',LGBM)],voting = 'soft')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h_W13Lc-p82"
      },
      "source": [
        "## ì •ë‹µ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om7UEOVZTUdU"
      },
      "source": [
        "# Xì— í•™ìŠµí•  ë°ì´í„°ë¥¼, yì— ëª©í‘œ ë³€ìˆ˜ë¥¼ ì €ì¥í•´ì£¼ì„¸ìš”\n",
        "\n",
        "X = train.drop(columns = ['index', 'quality'])\n",
        "\n",
        "y = train['quality']\n",
        "\n",
        "\n",
        "\n",
        "# ëœë¤í¬ë ˆìŠ¤íŠ¸ì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì˜ ë²”ìœ„ë¥¼ dictionary í˜•íƒœë¡œ ì§€ì •í•´ì£¼ì„¸ìš”\n",
        "\n",
        "## KeyëŠ” ëœë¤í¬ë ˆìŠ¤íŠ¸ì˜ hyperparameterì´ë¦„ì´ê³ , valueëŠ” íƒìƒ‰í•  ë²”ìœ„ ì…ë‹ˆë‹¤.\n",
        "\n",
        "rf_parameter_bounds = {\n",
        "\n",
        "                      'max_depth' : (1,3), # ë‚˜ë¬´ì˜ ê¹Šì´\n",
        "\n",
        "                      'n_estimators' : (30,100),\n",
        "\n",
        "                      }\n",
        "\n",
        "\n",
        "\n",
        "# í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì£¼ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "# í•¨ìˆ˜ì˜ êµ¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "# 1. í•¨ìˆ˜ì— ë“¤ì–´ê°€ëŠ” ì¸ì = ìœ„ì—ì„œ ë§Œë“  í•¨ìˆ˜ì˜ keyê°’ë“¤\n",
        "# 2. í•¨ìˆ˜ ì† ì¸ìë¥¼ í†µí•´ ë°›ì•„ì™€ ìƒˆë¡­ê²Œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
        "# 3. ê·¸ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ ìƒì„±\n",
        "# 4. train_test_splitì„ í†µí•´ ë°ì´í„° train-valid ë‚˜ëˆ„ê¸°\n",
        "# 5 .ëª¨ë¸ í•™ìŠµ\n",
        "# 6. ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •\n",
        "# 7. ëª¨ë¸ì˜ ì ìˆ˜ ë°˜í™˜\n",
        "\n",
        "\n",
        "\n",
        "def rf_bo(max_depth, n_estimators):\n",
        "\n",
        "  rf_params = {\n",
        "              'max_depth' : int(round(max_depth)),\n",
        "               'n_estimators' : int(round(n_estimators)),      \n",
        "              }\n",
        "\n",
        "  rf = RandomForestClassifier(**rf_params)\n",
        "\n",
        "\n",
        "\n",
        "  X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size = 0.2, )\n",
        "\n",
        "\n",
        "\n",
        "  rf.fit(X_train,y_train)\n",
        "\n",
        "  score = accuracy_score(y_valid, rf.predict(X_valid))\n",
        "\n",
        "  return score\n",
        "\n",
        "\n",
        "\n",
        "# ì´ì œ Bayesian Optimizationì„ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ëë‚¬ìŠµë‹ˆë‹¤.\n",
        "\n",
        "# \"BO_rf\"ë¼ëŠ” ë³€ìˆ˜ì— Bayesian Optmizationì„ ì €ì¥í•´ë³´ì„¸ìš”\n",
        "\n",
        "BO_rf = BayesianOptimization(f = rf_bo, pbounds = rf_parameter_bounds,random_state = 0)\n",
        "\n",
        "\n",
        "\n",
        "# Bayesian Optimizationì„ ì‹¤í–‰í•´ë³´ì„¸ìš”\n",
        "\n",
        "BO_rf.maximize(init_points = 5, n_iter = 5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwLo48x8TUoP"
      },
      "source": [
        "# Xì— í•™ìŠµí•  ë°ì´í„°ë¥¼, yì— ëª©í‘œ ë³€ìˆ˜ë¥¼ ì €ì¥í•´ì£¼ì„¸ìš”\n",
        "\n",
        "X = train.drop(columns = ['index', 'quality'])\n",
        "\n",
        "y = train['quality']\n",
        "\n",
        "\n",
        "\n",
        "# XGBoostì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì˜ ë²”ìœ„ë¥¼ dictionary í˜•íƒœë¡œ ì§€ì •í•´ì£¼ì„¸ìš”\n",
        "\n",
        "## KeyëŠ” XGBoost hyperparameterì´ë¦„ì´ê³ , valueëŠ” íƒìƒ‰í•  ë²”ìœ„ ì…ë‹ˆë‹¤.\n",
        "\n",
        "xgb_parameter_bounds = {\n",
        "                      'gamma' : (0,10),\n",
        "                      'max_depth' : (1,3), \n",
        "                      'subsample' : (0.5,1)\n",
        "                      }\n",
        "\n",
        "\n",
        "\n",
        "# í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì£¼ê² ìŠµë‹ˆë‹¤.\n",
        "# í•¨ìˆ˜ì˜ êµ¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "# 1. í•¨ìˆ˜ì— ë“¤ì–´ê°€ëŠ” ì¸ì = ìœ„ì—ì„œ ë§Œë“  í•¨ìˆ˜ì˜ keyê°’ë“¤\n",
        "# 2. í•¨ìˆ˜ ì† ì¸ìë¥¼ í†µí•´ ë°›ì•„ì™€ ìƒˆë¡­ê²Œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
        "# 3. ê·¸ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ ìƒì„±\n",
        "# 4. train_test_splitì„ í†µí•´ ë°ì´í„° train-valid ë‚˜ëˆ„ê¸°\n",
        "# 5 .ëª¨ë¸ í•™ìŠµ\n",
        "# 6. ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •\n",
        "# 7. ëª¨ë¸ì˜ ì ìˆ˜ ë°˜í™˜\n",
        "\n",
        "\n",
        "\n",
        "def xgb_bo(gamma,max_depth, subsample):\n",
        "\n",
        "  xgb_params = {\n",
        "              'gamma' : int(round(gamma)),\n",
        "              'max_depth' : int(round(max_depth)),\n",
        "               'subsample' : int(round(subsample)),      \n",
        "              }\n",
        "\n",
        "  xgb = XGBClassifier(**xgb_params)\n",
        "\n",
        "\n",
        "\n",
        "  X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size = 0.2, )\n",
        "\n",
        "\n",
        "\n",
        "  xgb.fit(X_train,y_train)\n",
        "  score = accuracy_score(y_valid, xgb.predict(X_valid))\n",
        "  return score\n",
        "\n",
        "\n",
        "\n",
        "# ì´ì œ Bayesian Optimizationì„ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ëë‚¬ìŠµë‹ˆë‹¤.\n",
        "# \"BO_xgb\"ë¼ëŠ” ë³€ìˆ˜ì— Bayesian Optmizationì„ ì €ì¥í•´ë³´ì„¸ìš”\n",
        "\n",
        "BO_xgb = BayesianOptimization(f = xgb_bo, pbounds = xgb_parameter_bounds,random_state = 0)\n",
        "\n",
        "\n",
        "\n",
        "# Bayesian Optimizationì„ ì‹¤í–‰í•´ë³´ì„¸ìš”\n",
        "\n",
        "BO_xgb.maximize(init_points = 5, n_iter = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xflEKoLQTUvP"
      },
      "source": [
        "# Xì— í•™ìŠµí•  ë°ì´í„°ë¥¼, yì— ëª©í‘œ ë³€ìˆ˜ë¥¼ ì €ì¥í•´ì£¼ì„¸ìš”\n",
        "\n",
        "X = train.drop(columns = ['index', 'quality'])\n",
        "\n",
        "y = train['quality']\n",
        "\n",
        "\n",
        "\n",
        "# LGBMì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì˜ ë²”ìœ„ë¥¼ dictionary í˜•íƒœë¡œ ì§€ì •í•´ì£¼ì„¸ìš”\n",
        "\n",
        "## KeyëŠ” LGBM hyperparameterì´ë¦„ì´ê³ , valueëŠ” íƒìƒ‰í•  ë²”ìœ„ ì…ë‹ˆë‹¤.\n",
        "\n",
        "lgbm_parameter_bounds = {\n",
        "                      'n_estimators' : (30,100),\n",
        "                      'max_depth' : (1,3), # ë‚˜ë¬´ì˜ ê¹Šì´\n",
        "                      'subsample' : (0.5,1)\n",
        "                      }\n",
        "\n",
        "\n",
        "\n",
        "# í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì£¼ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "# í•¨ìˆ˜ì˜ êµ¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "# 1. í•¨ìˆ˜ì— ë“¤ì–´ê°€ëŠ” ì¸ì = ìœ„ì—ì„œ ë§Œë“  í•¨ìˆ˜ì˜ keyê°’ë“¤\n",
        "# 2. í•¨ìˆ˜ ì† ì¸ìë¥¼ í†µí•´ ë°›ì•„ì™€ ìƒˆë¡­ê²Œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
        "# 3. ê·¸ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨ë¸ ìƒì„±\n",
        "# 4. train_test_splitì„ í†µí•´ ë°ì´í„° train-valid ë‚˜ëˆ„ê¸°\n",
        "# 5 .ëª¨ë¸ í•™ìŠµ\n",
        "# 6. ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •\n",
        "# 7. ëª¨ë¸ì˜ ì ìˆ˜ ë°˜í™˜\n",
        "\n",
        "\n",
        "\n",
        "def lgbm_bo(n_estimators,max_depth, subsample):\n",
        "\n",
        "  lgbm_params = {\n",
        "              'n_estimators' : int(round(n_estimators)),\n",
        "              'max_depth' : int(round(max_depth)),\n",
        "               'subsample' : int(round(subsample)),      \n",
        "              }\n",
        "\n",
        "  lgbm = LGBMClassifier(**lgbm_params)\n",
        "\n",
        "\n",
        "\n",
        "  X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size = 0.2, )\n",
        "\n",
        "\n",
        "\n",
        "  lgbm.fit(X_train,y_train)\n",
        "\n",
        "  score = accuracy_score(y_valid, lgbm.predict(X_valid))\n",
        "\n",
        "  return score\n",
        "\n",
        "\n",
        "\n",
        "# ì´ì œ Bayesian Optimizationì„ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ëë‚¬ìŠµë‹ˆë‹¤.\n",
        "\n",
        "# \"BO_lgbm\"ë¼ëŠ” ë³€ìˆ˜ì— Bayesian Optmizationì„ ì €ì¥í•´ë³´ì„¸ìš”\n",
        "\n",
        "BO_lgbm = BayesianOptimization(f = lgbm_bo, pbounds = lgbm_parameter_bounds,random_state = 0)\n",
        "\n",
        "\n",
        "\n",
        "# Bayesian Optimizationì„ ì‹¤í–‰í•´ë³´ì„¸ìš”\n",
        "\n",
        "BO_lgbm.maximize(init_points = 5, n_iter = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkfaG2k0TSxd"
      },
      "source": [
        "# ëª¨ë¸ ì •ì˜ (íŠœë‹ëœ íŒŒë¼ë¯¸í„°ë¡œ)\n",
        "LGBM = LGBMClassifier(max_depth = 2.09,n_estimators=60, subsample = 0.8229)\n",
        "XGB = XGBClassifier(gamma =  4.376, max_depth = 2.784, subsample = 0.9818)\n",
        "RF = RandomForestClassifier(max_depth = 3.0, n_estimators = 35.31)\n",
        "\n",
        "# VotingClassifier ì •ì˜\n",
        "VC = VotingClassifier(estimators=[('rf',RF),('xgb',XGB),('lgbm',LGBM)],voting = 'soft')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHM3-Ox-TUL8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8YgC54VtiB7"
      },
      "source": [
        "# [â†©ï¸ ë¦¬ìŠ¤íŠ¸ë¡œ ëŒì•„ê°€ê¸°](https://dacon.io/competitions/open/235698/overview/description)"
      ]
    }
  ]
}