{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lv4 튜닝 5/7 모델 튜닝 및 Voting Classifier",
      "provenance": [],
      "collapsed_sections": [
        "gU02qWuqR24f",
        "KJVABGt5SfYg",
        "_05EetCjSvoe",
        "_h_W13Lc-p82"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xoyeon/Dacon-Daily-Python-Camp/blob/main/Lv4_%ED%8A%9C%EB%8B%9D_5_7_%EB%AA%A8%EB%8D%B8_%ED%8A%9C%EB%8B%9D_%EB%B0%8F_Voting_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIpjpOEXsoW0"
      },
      "source": [
        "# [↩️ 리스트로 돌아가기](https://dacon.io/competitions/open/235698/overview/description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdkhATahdXka"
      },
      "source": [
        "# 셀을 실행할 때 팝업이 뜨면 `무시하고 계속하기` 를 눌러주세요\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91j6o0CC8y6w"
      },
      "source": [
        "## 데이터 다운로드\n",
        "---\n",
        "아래 셀을 실행시켜 데이터를 colab 에 불러옵니다.\n",
        "셀 실행은 Ctrl + Enter 를 이용해 실행시킬 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVPJmIjs82WW",
        "outputId": "71bff78a-0375-41ca-f396-c57a1e7732ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 데이터 다운로드 링크로 데이터를 코랩에 불러옵니다.\n",
        "\n",
        "!wget 'https://bit.ly/3i4n1QB'\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile('3i4n1QB', 'r') as existing_zip:\n",
        "    existing_zip.extractall('data')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-06 10:52:22--  https://bit.ly/3i4n1QB\n",
            "Resolving bit.ly (bit.ly)... 67.199.248.11, 67.199.248.10\n",
            "Connecting to bit.ly (bit.ly)|67.199.248.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://drive.google.com/uc?export=download&id=1emLrrpFWT8dCoj5BJb12-5QMG2-nruUw [following]\n",
            "--2021-09-06 10:52:22--  https://drive.google.com/uc?export=download&id=1emLrrpFWT8dCoj5BJb12-5QMG2-nruUw\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.13.238, 2607:f8b0:4004:80a::200e\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.13.238|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-10-10-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/702b8n79tqenn4fftk01n1neems64o51/1630925475000/17946651057176172524/*/1emLrrpFWT8dCoj5BJb12-5QMG2-nruUw?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-09-06 10:52:23--  https://doc-10-10-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/702b8n79tqenn4fftk01n1neems64o51/1630925475000/17946651057176172524/*/1emLrrpFWT8dCoj5BJb12-5QMG2-nruUw?e=download\n",
            "Resolving doc-10-10-docs.googleusercontent.com (doc-10-10-docs.googleusercontent.com)... 142.250.65.65, 2607:f8b0:4004:832::2001\n",
            "Connecting to doc-10-10-docs.googleusercontent.com (doc-10-10-docs.googleusercontent.com)|142.250.65.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 137694 (134K) [application/zip]\n",
            "Saving to: ‘3i4n1QB’\n",
            "\n",
            "3i4n1QB             100%[===================>] 134.47K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-09-06 10:52:23 (5.65 MB/s) - ‘3i4n1QB’ saved [137694/137694]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqVbFVMCGJzT",
        "outputId": "382c9133-f3d6-4184-bf1c-b5d8a85fdb49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "# 라이브러리 및 데이터 불러오기\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# 데이터를 불러와 학습시킬 준비하기\n",
        "\n",
        "train = pd.read_csv('data/train.csv')\n",
        "test = pd.read_csv('data/test.csv')\n",
        "\n",
        "# Scailing\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train[['fixed acidity']])\n",
        "train['Scaled fixed acidity'] = scaler.transform(train[['fixed acidity']])\n",
        "test['Scaled fixed acidity'] = scaler.transform(test[['fixed acidity']])\n",
        "\n",
        "# Encoding\n",
        "encoder = OneHotEncoder()\n",
        "encoder.fit(train[['type']])\n",
        "onehot = encoder.transform(train[['type']])\n",
        "onehot = onehot.toarray()\n",
        "onehot = pd.DataFrame(onehot)\n",
        "onehot.columns = encoder.get_feature_names()\n",
        "train = pd.concat([train, onehot], axis = 1)\n",
        "train = train.drop(columns = ['type'])\n",
        "\n",
        "onehot = encoder.transform(test[['type']])\n",
        "onehot = onehot.toarray()\n",
        "onehot = pd.DataFrame(onehot)\n",
        "onehot.columns = encoder.get_feature_names()\n",
        "test = pd.concat([test, onehot], axis = 1)\n",
        "test = test.drop(columns = ['type'])\n",
        "\n",
        "test.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>Scaled fixed acidity</th>\n",
              "      <th>x0_red</th>\n",
              "      <th>x0_white</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.48</td>\n",
              "      <td>6.6</td>\n",
              "      <td>0.043</td>\n",
              "      <td>11.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>0.99380</td>\n",
              "      <td>2.90</td>\n",
              "      <td>0.38</td>\n",
              "      <td>11.6</td>\n",
              "      <td>0.429752</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>13.3</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.58</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.070</td>\n",
              "      <td>15.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1.00040</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.49</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.785124</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>6.5</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.27</td>\n",
              "      <td>5.2</td>\n",
              "      <td>0.040</td>\n",
              "      <td>44.0</td>\n",
              "      <td>179.0</td>\n",
              "      <td>0.99480</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.69</td>\n",
              "      <td>9.4</td>\n",
              "      <td>0.223140</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>7.2</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.043</td>\n",
              "      <td>21.0</td>\n",
              "      <td>159.0</td>\n",
              "      <td>0.99480</td>\n",
              "      <td>3.52</td>\n",
              "      <td>0.47</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.280992</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>6.8</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.26</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.019</td>\n",
              "      <td>23.5</td>\n",
              "      <td>72.0</td>\n",
              "      <td>0.99041</td>\n",
              "      <td>3.16</td>\n",
              "      <td>0.47</td>\n",
              "      <td>11.8</td>\n",
              "      <td>0.247934</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  fixed acidity  ...  x0_red  x0_white\n",
              "0      0            9.0  ...     0.0       1.0\n",
              "1      1           13.3  ...     1.0       0.0\n",
              "2      2            6.5  ...     0.0       1.0\n",
              "3      3            7.2  ...     0.0       1.0\n",
              "4      4            6.8  ...     0.0       1.0\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWUuPtvUZ4zO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f46679dc-9a75-478a-f8d9-8b9d887c372e"
      },
      "source": [
        "pip install bayesian-optimization"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian-optimization-1.2.0.tar.gz (14 kB)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.0.1)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-py3-none-any.whl size=11685 sha256=855605f311a3409e5627c214bb541642ab42f6f9bc35599411ff1ab42f928a15\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/9b/71/f127d694e02eb40bcf18c7ae9613b88a6be4470f57a8528c5b\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5MhylK9Z7Ov"
      },
      "source": [
        "# Bayesian Optimization 불러오기\n",
        "from bayes_opt import BayesianOptimization"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZksEbPou52yW"
      },
      "source": [
        "## 👋 실습\n",
        "---\n",
        "실습 순서는 다음과 같습니다.\n",
        "1. Random forest 튜닝\n",
        "2. XGBoost 튜닝\n",
        "3. Light GBM 튜닝\n",
        "4. Voting Claasifier 생성\n",
        "5. Voting Claasifier 학습 및 예측\n",
        "---\n",
        "이번시간에는 실습 량이 많아 Voting Classifier 생성까지만 진행하고 다음 시간에 이어서 진행 할 예정입니다.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU02qWuqR24f"
      },
      "source": [
        "### Random forest 튜닝\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41v3TTXtR2uh",
        "outputId": "77b6c1a0-7329-4353-ba4e-f84a566e5b2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# X에 학습할 데이터를, y에 목표 변수를 저장해주세요\n",
        "\n",
        "X = train.drop(columns = ['index', 'quality'])\n",
        "\n",
        "y = train['quality']\n",
        "\n",
        "\n",
        "\n",
        "# 랜덤포레스트의 하이퍼 파라미터의 범위를 dictionary 형태로 지정해주세요\n",
        "\n",
        "## Key는 랜덤포레스트의 hyperparameter이름이고, value는 탐색할 범위 입니다.\n",
        "\n",
        "rf_parameter_bounds = {\n",
        "\n",
        "                      'max_depth' : (1,3), # 나무의 깊이\n",
        "\n",
        "                      'n_estimators' : (30,100),\n",
        "\n",
        "                      }\n",
        "\n",
        "\n",
        "\n",
        "# 함수를 만들어주겠습니다.\n",
        "\n",
        "# 함수의 구성은 다음과 같습니다.\n",
        "\n",
        "# 1. 함수에 들어가는 인자 = 위에서 만든 함수의 key값들\n",
        "\n",
        "# 2. 함수 속 인자를 통해 받아와 새롭게 하이퍼파라미터 딕셔너리 생성\n",
        "\n",
        "# 3. 그 딕셔너리를 바탕으로 모델 생성\n",
        "\n",
        "# 4. train_test_split을 통해 데이터 train-valid 나누기\n",
        "\n",
        "# 5 .모델 학습\n",
        "\n",
        "# 6. 모델 성능 측정\n",
        "\n",
        "# 7. 모델의 점수 반환\n",
        "\n",
        "\n",
        "\n",
        "def rf_bo(max_depth, n_estimators):\n",
        "  #####################################\n",
        " \n",
        "  rf_params = {\n",
        "\n",
        "              'max_depth' : int(round(max_depth)),\n",
        "\n",
        "               'n_estimators' : int(round(n_estimators)),      \n",
        "\n",
        "              }\n",
        "\n",
        "  rf = RandomForestClassifier(**rf_params)\n",
        "\n",
        "\n",
        "\n",
        "  X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size = 0.2, )\n",
        "\n",
        "\n",
        "\n",
        "  rf.fit(X_train,y_train)\n",
        "\n",
        "  score = accuracy_score(y_valid, rf.predict(X_valid))\n",
        "\n",
        "  return score\n",
        "\n",
        "  #####################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 이제 Bayesian Optimization을 사용할 준비가 끝났습니다.\n",
        "\n",
        "# \"BO_rf\"라는 변수에 Bayesian Optmization을 저장해보세요\n",
        "\n",
        "BO_rf = BayesianOptimization(f = rf_bo, pbounds = rf_parameter_bounds,random_state = 0)\n",
        "\n",
        "\n",
        "\n",
        "# Bayesian Optimization을 실행해보세요\n",
        "\n",
        "BO_rf.maximize(init_points = 5, n_iter = 5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | max_depth | n_esti... |\n",
            "-------------------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5073  \u001b[0m | \u001b[0m 2.098   \u001b[0m | \u001b[0m 80.06   \u001b[0m |\n",
            "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.5273  \u001b[0m | \u001b[95m 2.206   \u001b[0m | \u001b[95m 68.14   \u001b[0m |\n",
            "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.5155  \u001b[0m | \u001b[0m 1.847   \u001b[0m | \u001b[0m 75.21   \u001b[0m |\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5145  \u001b[0m | \u001b[0m 1.875   \u001b[0m | \u001b[0m 92.42   \u001b[0m |\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.5273  \u001b[0m | \u001b[0m 2.927   \u001b[0m | \u001b[0m 56.84   \u001b[0m |\n",
            "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.4291  \u001b[0m | \u001b[0m 1.106   \u001b[0m | \u001b[0m 30.01   \u001b[0m |\n",
            "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.4355  \u001b[0m | \u001b[0m 1.024   \u001b[0m | \u001b[0m 71.5    \u001b[0m |\n",
            "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.5355  \u001b[0m | \u001b[95m 3.0     \u001b[0m | \u001b[95m 100.0   \u001b[0m |\n",
            "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.5418  \u001b[0m | \u001b[95m 3.0     \u001b[0m | \u001b[95m 41.58   \u001b[0m |\n",
            "| \u001b[95m 10      \u001b[0m | \u001b[95m 0.5509  \u001b[0m | \u001b[95m 3.0     \u001b[0m | \u001b[95m 72.73   \u001b[0m |\n",
            "=================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJVABGt5SfYg"
      },
      "source": [
        "### XGBoost 튜닝"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pWpHl1LdeEp",
        "outputId": "84952271-bb1d-4f79-988f-dbe15523e14e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# X에 학습할 데이터를, y에 목표 변수를 저장해주세요\n",
        "\n",
        "X = train.drop(columns = ['index', 'quality'])\n",
        "\n",
        "y = train['quality']\n",
        "\n",
        "\n",
        "\n",
        "# XGBoost의 하이퍼 파라미터의 범위를 dictionary 형태로 지정해주세요\n",
        "\n",
        "## Key는 XGBoost hyperparameter이름이고, value는 탐색할 범위 입니다.\n",
        "\n",
        "xgb_parameter_bounds = {\n",
        "\n",
        "                      'gamma' : (0,10),\n",
        "\n",
        "                      'max_depth' : (1,3), # 나무의 깊이\n",
        "\n",
        "                      'subsample' : (0.5,1)\n",
        "\n",
        "                      }\n",
        "\n",
        "\n",
        "\n",
        "# 함수를 만들어주겠습니다.\n",
        "# 함수의 구성은 다음과 같습니다.\n",
        "# 1. 함수에 들어가는 인자 = 위에서 만든 함수의 key값들\n",
        "# 2. 함수 속 인자를 통해 받아와 새롭게 하이퍼파라미터 딕셔너리 생성\n",
        "# 3. 그 딕셔너리를 바탕으로 모델 생성\n",
        "# 4. train_test_split을 통해 데이터 train-valid 나누기\n",
        "# 5 .모델 학습\n",
        "# 6. 모델 성능 측정\n",
        "# 7. 모델의 점수 반환\n",
        "\n",
        "\n",
        "\n",
        "def xgb_bo(gamma,max_depth, subsample):\n",
        "\n",
        "  #####################################\n",
        "  xgb_params = {\n",
        "\n",
        "              'gamma' : int(round(gamma)),\n",
        "\n",
        "              'max_depth' : int(round(max_depth)),\n",
        "\n",
        "               'subsample' : int(round(subsample)),      \n",
        "\n",
        "              }\n",
        "  xgb = XGBClassifier(**xgb_params)\n",
        "\n",
        "\n",
        "\n",
        "  X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size = 0.2, )\n",
        "\n",
        "\n",
        "\n",
        "  xgb.fit(X_train,y_train)\n",
        "\n",
        "  score = accuracy_score(y_valid, xgb.predict(X_valid))\n",
        "\n",
        "  return score\n",
        "  #####################################\n",
        "\n",
        "\n",
        "\n",
        "# 이제 Bayesian Optimization을 사용할 준비가 끝났습니다.\n",
        "\n",
        "# \"BO_xgb\"라는 변수에 Bayesian Optmization을 저장해보세요\n",
        "\n",
        "BO_xgb = BayesianOptimization(f = xgb_bo, pbounds = xgb_parameter_bounds,random_state = 0)\n",
        "\n",
        "\n",
        "\n",
        "# Bayesian Optimization을 실행해보세요\n",
        "\n",
        "BO_xgb.maximize(init_points = 5, n_iter = 5)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   |   gamma   | max_depth | subsample |\n",
            "-------------------------------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5309  \u001b[0m | \u001b[0m 5.488   \u001b[0m | \u001b[0m 2.43    \u001b[0m | \u001b[0m 0.8014  \u001b[0m |\n",
            "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.5255  \u001b[0m | \u001b[0m 5.449   \u001b[0m | \u001b[0m 1.847   \u001b[0m | \u001b[0m 0.8229  \u001b[0m |\n",
            "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.5691  \u001b[0m | \u001b[95m 4.376   \u001b[0m | \u001b[95m 2.784   \u001b[0m | \u001b[95m 0.9818  \u001b[0m |\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5445  \u001b[0m | \u001b[0m 3.834   \u001b[0m | \u001b[0m 2.583   \u001b[0m | \u001b[0m 0.7644  \u001b[0m |\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.5573  \u001b[0m | \u001b[0m 5.68    \u001b[0m | \u001b[0m 2.851   \u001b[0m | \u001b[0m 0.5355  \u001b[0m |\n",
            "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.000909\u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m |\n",
            "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.5445  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
            "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.5891  \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 3.0     \u001b[0m | \u001b[95m 1.0     \u001b[0m |\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.5382  \u001b[0m | \u001b[0m 2.343   \u001b[0m | \u001b[0m 1.026   \u001b[0m | \u001b[0m 0.9694  \u001b[0m |\n",
            "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.004545\u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 3.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m |\n",
            "=============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_05EetCjSvoe"
      },
      "source": [
        "### LGBM 튜닝\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qH2nrvlSx6I",
        "outputId": "c813afd2-c6a5-4dbb-e406-a35e52be8bca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# X에 학습할 데이터를, y에 목표 변수를 저장해주세요\n",
        "\n",
        "X = train.drop(columns = ['index', 'quality'])\n",
        "\n",
        "y = train['quality']\n",
        "\n",
        "\n",
        "\n",
        "# LGBM의 하이퍼 파라미터의 범위를 dictionary 형태로 지정해주세요\n",
        "\n",
        "## Key는 LGBM hyperparameter이름이고, value는 탐색할 범위 입니다.\n",
        "\n",
        "lgbm_parameter_bounds = {\n",
        "\n",
        "                      'n_estimators' : (30,100),\n",
        "\n",
        "                      'max_depth' : (1,3), # 나무의 깊이\n",
        "\n",
        "                      'subsample' : (0.5,1)\n",
        "\n",
        "                      }\n",
        "\n",
        "\n",
        "\n",
        "# 함수를 만들어주겠습니다.\n",
        "\n",
        "# 함수의 구성은 다음과 같습니다.\n",
        "\n",
        "# 1. 함수에 들어가는 인자 = 위에서 만든 함수의 key값들\n",
        "\n",
        "# 2. 함수 속 인자를 통해 받아와 새롭게 하이퍼파라미터 딕셔너리 생성\n",
        "\n",
        "# 3. 그 딕셔너리를 바탕으로 모델 생성\n",
        "\n",
        "# 4. train_test_split을 통해 데이터 train-valid 나누기\n",
        "\n",
        "# 5. 모델 학습\n",
        "\n",
        "# 6. 모델 성능 측정\n",
        "\n",
        "# 7. 모델의 점수 반환\n",
        "\n",
        "\n",
        "\n",
        "def lgbm_bo(n_estimators,max_depth, subsample):\n",
        "\n",
        "  #####################################\n",
        "  lgbm_params = {\n",
        "\n",
        "              'n_estimators' : int(round(n_estimators)),\n",
        "\n",
        "              'max_depth' : int(round(max_depth)),\n",
        "\n",
        "               'subsample' : int(round(subsample)),      \n",
        "\n",
        "              }\n",
        "  lgbm = LGBMClassifier(**lgbm_params)\n",
        "\n",
        "\n",
        "\n",
        "  X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size = 0.2, )\n",
        "\n",
        "\n",
        "\n",
        "  lgbm.fit(X_train,y_train)\n",
        "\n",
        "  score = accuracy_score(y_valid, lgbm.predict(X_valid))\n",
        "\n",
        "  return score\n",
        "\n",
        "\n",
        "  #####################################\n",
        "\n",
        "\n",
        "\n",
        "# 이제 Bayesian Optimization을 사용할 준비가 끝났습니다.\n",
        "\n",
        "# \"BO_lgbm\"라는 변수에 Bayesian Optmization을 저장해보세요\n",
        "\n",
        "BO_lgbm = BayesianOptimization(f = lgbm_bo, pbounds = lgbm_parameter_bounds,random_state = 0)\n",
        "\n",
        "\n",
        "\n",
        "# Bayesian Optimization을 실행해보세요\n",
        "BO_lgbm.maximize(init_points = 5, n_iter = 5)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | max_depth | n_esti... | subsample |\n",
            "-------------------------------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.56    \u001b[0m | \u001b[0m 2.098   \u001b[0m | \u001b[0m 80.06   \u001b[0m | \u001b[0m 0.8014  \u001b[0m |\n",
            "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.5591  \u001b[0m | \u001b[0m 2.09    \u001b[0m | \u001b[0m 59.66   \u001b[0m | \u001b[0m 0.8229  \u001b[0m |\n",
            "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.5409  \u001b[0m | \u001b[0m 1.875   \u001b[0m | \u001b[0m 92.42   \u001b[0m | \u001b[0m 0.9818  \u001b[0m |\n",
            "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5464  \u001b[0m | \u001b[0m 1.767   \u001b[0m | \u001b[0m 85.42   \u001b[0m | \u001b[0m 0.7644  \u001b[0m |\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.5309  \u001b[0m | \u001b[0m 2.136   \u001b[0m | \u001b[0m 94.79   \u001b[0m | \u001b[0m 0.5355  \u001b[0m |\n",
            "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.5445  \u001b[0m | \u001b[0m 2.951   \u001b[0m | \u001b[0m 30.14   \u001b[0m | \u001b[0m 0.9851  \u001b[0m |\n",
            "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.5636  \u001b[0m | \u001b[95m 2.975   \u001b[0m | \u001b[95m 70.17   \u001b[0m | \u001b[95m 0.9819  \u001b[0m |\n",
            "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.5345  \u001b[0m | \u001b[0m 1.092   \u001b[0m | \u001b[0m 42.31   \u001b[0m | \u001b[0m 0.9997  \u001b[0m |\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.5545  \u001b[0m | \u001b[0m 1.004   \u001b[0m | \u001b[0m 70.16   \u001b[0m | \u001b[0m 0.9359  \u001b[0m |\n",
            "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.5609  \u001b[0m | \u001b[0m 2.995   \u001b[0m | \u001b[0m 51.18   \u001b[0m | \u001b[0m 0.883   \u001b[0m |\n",
            "=============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF3qStQlS-G3"
      },
      "source": [
        "### Voting Classifier 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMd_ySPETMcr"
      },
      "source": [
        "# 모델 정의 (튜닝된 파라미터로)\n",
        "LGBM = LGBMClassifier(max_depth = 2.09,n_estimators=60, subsample = 0.8229)\n",
        "XGB = XGBClassifier(gamma =  4.376, max_depth = 2.784, subsample = 0.9818)\n",
        "RF = RandomForestClassifier(max_depth = 3.0, n_estimators = 35.31)\n",
        "\n",
        "# VotingClassifier 정의\n",
        "VC = VotingClassifier(estimators=[('rf',RF),('xgb',XGB),('lgbm',LGBM)],voting = 'soft')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h_W13Lc-p82"
      },
      "source": [
        "## 정답"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om7UEOVZTUdU"
      },
      "source": [
        "# X에 학습할 데이터를, y에 목표 변수를 저장해주세요\n",
        "\n",
        "X = train.drop(columns = ['index', 'quality'])\n",
        "\n",
        "y = train['quality']\n",
        "\n",
        "\n",
        "\n",
        "# 랜덤포레스트의 하이퍼 파라미터의 범위를 dictionary 형태로 지정해주세요\n",
        "\n",
        "## Key는 랜덤포레스트의 hyperparameter이름이고, value는 탐색할 범위 입니다.\n",
        "\n",
        "rf_parameter_bounds = {\n",
        "\n",
        "                      'max_depth' : (1,3), # 나무의 깊이\n",
        "\n",
        "                      'n_estimators' : (30,100),\n",
        "\n",
        "                      }\n",
        "\n",
        "\n",
        "\n",
        "# 함수를 만들어주겠습니다.\n",
        "\n",
        "# 함수의 구성은 다음과 같습니다.\n",
        "# 1. 함수에 들어가는 인자 = 위에서 만든 함수의 key값들\n",
        "# 2. 함수 속 인자를 통해 받아와 새롭게 하이퍼파라미터 딕셔너리 생성\n",
        "# 3. 그 딕셔너리를 바탕으로 모델 생성\n",
        "# 4. train_test_split을 통해 데이터 train-valid 나누기\n",
        "# 5 .모델 학습\n",
        "# 6. 모델 성능 측정\n",
        "# 7. 모델의 점수 반환\n",
        "\n",
        "\n",
        "\n",
        "def rf_bo(max_depth, n_estimators):\n",
        "\n",
        "  rf_params = {\n",
        "              'max_depth' : int(round(max_depth)),\n",
        "               'n_estimators' : int(round(n_estimators)),      \n",
        "              }\n",
        "\n",
        "  rf = RandomForestClassifier(**rf_params)\n",
        "\n",
        "\n",
        "\n",
        "  X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size = 0.2, )\n",
        "\n",
        "\n",
        "\n",
        "  rf.fit(X_train,y_train)\n",
        "\n",
        "  score = accuracy_score(y_valid, rf.predict(X_valid))\n",
        "\n",
        "  return score\n",
        "\n",
        "\n",
        "\n",
        "# 이제 Bayesian Optimization을 사용할 준비가 끝났습니다.\n",
        "\n",
        "# \"BO_rf\"라는 변수에 Bayesian Optmization을 저장해보세요\n",
        "\n",
        "BO_rf = BayesianOptimization(f = rf_bo, pbounds = rf_parameter_bounds,random_state = 0)\n",
        "\n",
        "\n",
        "\n",
        "# Bayesian Optimization을 실행해보세요\n",
        "\n",
        "BO_rf.maximize(init_points = 5, n_iter = 5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwLo48x8TUoP"
      },
      "source": [
        "# X에 학습할 데이터를, y에 목표 변수를 저장해주세요\n",
        "\n",
        "X = train.drop(columns = ['index', 'quality'])\n",
        "\n",
        "y = train['quality']\n",
        "\n",
        "\n",
        "\n",
        "# XGBoost의 하이퍼 파라미터의 범위를 dictionary 형태로 지정해주세요\n",
        "\n",
        "## Key는 XGBoost hyperparameter이름이고, value는 탐색할 범위 입니다.\n",
        "\n",
        "xgb_parameter_bounds = {\n",
        "                      'gamma' : (0,10),\n",
        "                      'max_depth' : (1,3), \n",
        "                      'subsample' : (0.5,1)\n",
        "                      }\n",
        "\n",
        "\n",
        "\n",
        "# 함수를 만들어주겠습니다.\n",
        "# 함수의 구성은 다음과 같습니다.\n",
        "# 1. 함수에 들어가는 인자 = 위에서 만든 함수의 key값들\n",
        "# 2. 함수 속 인자를 통해 받아와 새롭게 하이퍼파라미터 딕셔너리 생성\n",
        "# 3. 그 딕셔너리를 바탕으로 모델 생성\n",
        "# 4. train_test_split을 통해 데이터 train-valid 나누기\n",
        "# 5 .모델 학습\n",
        "# 6. 모델 성능 측정\n",
        "# 7. 모델의 점수 반환\n",
        "\n",
        "\n",
        "\n",
        "def xgb_bo(gamma,max_depth, subsample):\n",
        "\n",
        "  xgb_params = {\n",
        "              'gamma' : int(round(gamma)),\n",
        "              'max_depth' : int(round(max_depth)),\n",
        "               'subsample' : int(round(subsample)),      \n",
        "              }\n",
        "\n",
        "  xgb = XGBClassifier(**xgb_params)\n",
        "\n",
        "\n",
        "\n",
        "  X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size = 0.2, )\n",
        "\n",
        "\n",
        "\n",
        "  xgb.fit(X_train,y_train)\n",
        "  score = accuracy_score(y_valid, xgb.predict(X_valid))\n",
        "  return score\n",
        "\n",
        "\n",
        "\n",
        "# 이제 Bayesian Optimization을 사용할 준비가 끝났습니다.\n",
        "# \"BO_xgb\"라는 변수에 Bayesian Optmization을 저장해보세요\n",
        "\n",
        "BO_xgb = BayesianOptimization(f = xgb_bo, pbounds = xgb_parameter_bounds,random_state = 0)\n",
        "\n",
        "\n",
        "\n",
        "# Bayesian Optimization을 실행해보세요\n",
        "\n",
        "BO_xgb.maximize(init_points = 5, n_iter = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xflEKoLQTUvP"
      },
      "source": [
        "# X에 학습할 데이터를, y에 목표 변수를 저장해주세요\n",
        "\n",
        "X = train.drop(columns = ['index', 'quality'])\n",
        "\n",
        "y = train['quality']\n",
        "\n",
        "\n",
        "\n",
        "# LGBM의 하이퍼 파라미터의 범위를 dictionary 형태로 지정해주세요\n",
        "\n",
        "## Key는 LGBM hyperparameter이름이고, value는 탐색할 범위 입니다.\n",
        "\n",
        "lgbm_parameter_bounds = {\n",
        "                      'n_estimators' : (30,100),\n",
        "                      'max_depth' : (1,3), # 나무의 깊이\n",
        "                      'subsample' : (0.5,1)\n",
        "                      }\n",
        "\n",
        "\n",
        "\n",
        "# 함수를 만들어주겠습니다.\n",
        "\n",
        "# 함수의 구성은 다음과 같습니다.\n",
        "# 1. 함수에 들어가는 인자 = 위에서 만든 함수의 key값들\n",
        "# 2. 함수 속 인자를 통해 받아와 새롭게 하이퍼파라미터 딕셔너리 생성\n",
        "# 3. 그 딕셔너리를 바탕으로 모델 생성\n",
        "# 4. train_test_split을 통해 데이터 train-valid 나누기\n",
        "# 5 .모델 학습\n",
        "# 6. 모델 성능 측정\n",
        "# 7. 모델의 점수 반환\n",
        "\n",
        "\n",
        "\n",
        "def lgbm_bo(n_estimators,max_depth, subsample):\n",
        "\n",
        "  lgbm_params = {\n",
        "              'n_estimators' : int(round(n_estimators)),\n",
        "              'max_depth' : int(round(max_depth)),\n",
        "               'subsample' : int(round(subsample)),      \n",
        "              }\n",
        "\n",
        "  lgbm = LGBMClassifier(**lgbm_params)\n",
        "\n",
        "\n",
        "\n",
        "  X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size = 0.2, )\n",
        "\n",
        "\n",
        "\n",
        "  lgbm.fit(X_train,y_train)\n",
        "\n",
        "  score = accuracy_score(y_valid, lgbm.predict(X_valid))\n",
        "\n",
        "  return score\n",
        "\n",
        "\n",
        "\n",
        "# 이제 Bayesian Optimization을 사용할 준비가 끝났습니다.\n",
        "\n",
        "# \"BO_lgbm\"라는 변수에 Bayesian Optmization을 저장해보세요\n",
        "\n",
        "BO_lgbm = BayesianOptimization(f = lgbm_bo, pbounds = lgbm_parameter_bounds,random_state = 0)\n",
        "\n",
        "\n",
        "\n",
        "# Bayesian Optimization을 실행해보세요\n",
        "\n",
        "BO_lgbm.maximize(init_points = 5, n_iter = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkfaG2k0TSxd"
      },
      "source": [
        "# 모델 정의 (튜닝된 파라미터로)\n",
        "LGBM = LGBMClassifier(max_depth = 2.09,n_estimators=60, subsample = 0.8229)\n",
        "XGB = XGBClassifier(gamma =  4.376, max_depth = 2.784, subsample = 0.9818)\n",
        "RF = RandomForestClassifier(max_depth = 3.0, n_estimators = 35.31)\n",
        "\n",
        "# VotingClassifier 정의\n",
        "VC = VotingClassifier(estimators=[('rf',RF),('xgb',XGB),('lgbm',LGBM)],voting = 'soft')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHM3-Ox-TUL8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8YgC54VtiB7"
      },
      "source": [
        "# [↩️ 리스트로 돌아가기](https://dacon.io/competitions/open/235698/overview/description)"
      ]
    }
  ]
}